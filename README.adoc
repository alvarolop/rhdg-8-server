= Red Hat Data Grid 8 server
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2020-11
// Metadata
:description: This document shows how to perform basic installations of Red Hat Data Grid customizing its configuration.
:keywords: infinispan, datagrid, openshift, red hat
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons


This repository demonstrates some of the basic features of Red Hat's latest Data Grid release and how to deploy a Data Grid cluster locally and on OCP. 

// Create the Table of contents here
toc::[]

== Introduction

Red Hat Data Grid is an in-memory, distributed, NoSQL datastore solution. Your applications can access, process, and analyze data at in-memory speed to deliver a superior user experience. 

Red Hat Data Grid provides value as a standard architectural component in application infrastructures for a variety of real-world scenarios and usecases:

* Data caching and transient data storage.
* Primary data store.
* Low latency compute grid.




== Deploying standalone

Currently, Data Grid is supported both using the binary on VMs and using the container images on Openshift. Due to the huge benefits of deploying our applications containerized in Openshift, we will use the same image to launch the Data Grid cluster locally using `podman-compose`:


.Launch the Data Grid cluster with only one replica
[source, bash]
----
podman-compose -f rhdg-local/docker-compose-1.yaml up
----

.Launch the Data Grid cluster with two replicas
[source, bash]
----
podman-compose -f rhdg-local/docker-compose-2.yaml up
----

If the cluster formation goes well, you will see that the JGroups Cluster View is updated to the two servers:

.Launch the server
[source, bash, subs="attributes"]
----
[eu-01] | 2023-06-27 07:48:06,213 INFO  (non-blocking-thread-infinispan-europe-01-p2-t1) [org.infinispan.LIFECYCLE] [Context=___hotRodTopologyCache_hotrod-default][Scope=infinispan-europe-01]ISPN100010: Finished rebalance with members [infinispan-europe-02, infinispan-europe-01], topology id 7
[eu-02] | 2023-06-27 07:48:06,224 INFO  (jgroups-5,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100009: Advancing to rebalance phase READ_ALL_WRITE_ALL, topology id 8
[eu-02] | 2023-06-27 07:48:06,227 INFO  (jgroups-6,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100009: Advancing to rebalance phase READ_NEW_WRITE_ALL, topology id 9
[eu-02] | 2023-06-27 07:48:06,230 INFO  (jgroups-5,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100010: Finished rebalance with members [infinispan-europe-02, infinispan-europe-01], topology id 10
//... 
[eu-01] | 2023-06-27 07:48:06,401 INFO  (main) [org.infinispan.SERVER] ISPN080001: Infinispan Server 14.0.1.Final started in 2777ms
----

NOTE: For ways of deploying the server using the .zip, check the https://github.com/alvarolop/rhdg8-server/tree/rhdg84-rhel[following version of this repository].





== Deploying RHDG on OCP using the Operator

Install Data Grid Operator into an OpenShift namespace to create and manage Data Grid clusters.

=== Deploying the RHDG operator

To deploy the RHDG operator, you will need to create four objects: The namespace of the Operator, the namespace where the Data Grid cluster will live, the Subscription and the Operator Group.


IMPORTANT: Bear in mind that you will need `cluster-admin` permissions to deploy an operator, as it is necessary to create cluster-wide CRDs (Custom Resource Definitions).

Execute the following command to deploy the operator: 

[source, bash]
----
oc process -f rhdg-operator/rhdg-01-operator.yaml -p OPERATOR_NAMESPACE="rhdg8-operator" -p CLUSTER_NAMESPACE="rhdg8" | oc apply -f -
----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_operator_guide/installation[documentation].


=== Deploying an RHDG cluster

The Data Grid operator defines the *Infinispan* CRD to simplify the process of creation, configuration, and management of DG clusters.


I have created several OCP templates to quickly deploy different set-ups of Data Grid:

[cols="4*",options="header",width=100%]
|===
|Template
|Authentication
|Authorization
|SSL Encryption

| link:rhdg-operator/rhdg-02-cluster-basic.yaml[Basic]
|{no}
|{no}
|{no}

| link:rhdg-operator/rhdg-02-cluster-auth.yaml[Auth]
|{yes}
|{yes}
|{no}

| link:rhdg-operator/rhdg-02-cluster-auth-ssl.yaml[SSL Encryption]
|{yes}
|{yes}
|{yes}

|===


Execute the following commands depending on the cluster that you want to deploy: 


[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster-basic.yaml -p CLUSTER_NAME="cluster-01" | oc apply -f -

oc process -f rhdg-operator/rhdg-02-cluster-auth.yaml -p CLUSTER_NAME="cluster-02" | oc apply -f -

oc process -f rhdg-operator/rhdg-02-cluster-auth-ssl.yaml -p CLUSTER_NAME="cluster-03" | oc apply -f -
----



=== Creating RHDG caches using the Operator CRD

Data Grid stores entries into caches, which can be created using several methods: REST, CLI, programmatically using the Java Client or using the `Cache` CRD. In this section, we will explore how to create caches using the Operator.


NOTE: For other ways of creating caches, please check this other https://github.com/alvarolop/rhdg8-client#cache-configuration[Git repository] with information about the Data Grid client.

To create caches with Data Grid Operator, you use Cache CRs to add caches from templates or XML configuration. Bear in mind that you can only create a single cache for each `Cache` CR.

I have created an OCP template to quickly set up two caches on the RHDG cluster:

* `operator-cache-01`: Based on an XML configuration.
* `operator-cache-02`: Based on a YAML configuration.
* `operator-cache-03`: Based on an already defined templated.


In order to apply this template, just execute the following command:
[source, bash]
----
oc process -f rhdg-operator/rhdg-03-caches.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster is installed and the name of the cluster to deploy. The cluster namespace should be the same as in the previous step. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`.
* *CLUSTER_NAME* = `rhdg`.


Modify them just by passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-03-caches.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----

Interact with the newly created caches with the following commands:
[source, bash]
----
# Set your variables. These are default:
CLUSTER_NAMESPACE="rhdg8"
CLUSTER_NAME="rhdg"
RHDG_URL=$(oc get route ${CLUSTER_NAME}-external -n ${CLUSTER_NAMESPACE} -o template='https://{{.spec.host}}')

# Check all the caches on your cluster
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches | jq

# Check information about an specific cache
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches/${CACHE_NAME} | jq

# Delete a cache
curl -X DELETE -k -u developer:developer ${RHDG_URL}/rest/v2/caches/${CACHE_NAME}
----


For more information about how to create caches using the CRD, please check the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/guide/e1fe5fa9-8bf4-4e0e-b6c1-ebef73539436[official documentation].


=== Monitoring RHDG with Prometheus

Data Grid exposes a metrics endpoint that provides statistics and events to Prometheus.

After installing OpenShift Container Platform 4.6, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects. You can then query metrics, review dashboards, and manage alerting rules and silences for your own projects in the OpenShift Container Platform web console. We are going to take advantage of this feature.


.Enabling monitoring for user-defined projects
[WARNING]
==== 
Monitoring of user-defined projects is not enabled by default. To enable it, you need to modify a ConfigMap of the `openshift-monitoring`. You need permission to create and modify ConfigMaps in this project. You only have to execute this command *once* per OCP cluster. [underline]#Please, do not execute it before checking if this was done before, you can override work from your colleagues#

[source, bash]
----
oc apply -f ocp/ocp-01-user-workload-monitoring.yaml
----

After executing the command above, you will see some pods in the following namespace:
[source, bash]
----
oc get pods -n openshift-user-workload-monitoring
----
====

// In order to access the Prometheus that will contain the metrics of DG, expose its service:
// [source, bash]
// ----
// oc expose svc/prometheus-user-workload -n openshift-user-workload-monitoring
// ----


I have created an OCP template to quickly configure metrics monitoring of an RHDG cluster. Execute the following command:

[source, bash]
----
oc process -f rhdg-operator/rhdg-04-monitoring.yaml | oc apply -f -
----

This template provides two parameters to modify the project where the cluster was installed and the name of the cluster itself. By default, values are: 

* *CLUSTER_NAMESPACE* = `rhdg8`
* *CLUSTER_NAME* = `rhdg`


Modify them just by passing arguments to the template:

[source, bash]
----
oc process -f rhdg-operator/rhdg-04-monitoring.yaml -p CLUSTER_NAMESPACE="another-namespace" -p CLUSTER_NAME="my-cluster" | oc apply -f -
----

For more information, access the Openshift https://docs.openshift.com/container-platform/4.9/monitoring/monitoring-overview.html[documentation] for the monitoring stack and the RHDG documentation to https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/guide/990f6af5-2a9c-4af1-afaf-d2ea562c5bf8[configure monitoring] for RHDG 8 on OCP.










== Deploying RHDG on OCP using Helm Charts

Helm is an application package manager for Kubernetes, which coordinates the download, installation, and deployment of apps. The original goal of Helm was to provide users with a better way to manage all the Kubernetes YAML files we create on Kubernetes projects using Helm Charts. A Chart is basically a set of templates and a file containing variables used to fill these templates. Let's have a look at an example.

=== Option 1: Using an official Infinispan Helm Chart release

In order to create your first deployment easily, first https://github.com/openshift-helm-charts/charts/blob/main/README.md[add the OpenShift Helm Charts repository]:


[source, bash]
----
helm repo add openshift-helm-charts https://charts.openshift.io/
----

Create a new OCP project:
[source, bash]
----
oc new-project rhdg8-helm --display-name="RHDG 8 - Helm" --description="This namespace contains a deployment of RHDG using the official Helm Chart"
----

Then, modify the `rhdg-chart/default-values.yaml` to configure your deployment:
[source, bash]
----
helm install rhdg openshift-helm-charts/redhat-data-grid -f rhdg-chart/minimal-values.yaml
----

You will be able to authenticate to the cluster using the credentials obtained from the following command:
[source, bash]
----
oc get secret rhdg-generated-secret \
-o jsonpath="{.data.identities-batch}" | base64 --decode
----

If you want to make changes, you need to update the values file and use the `helm upgrade` command:
[source, bash]
----
helm upgrade rhdg openshift-helm-charts/redhat-data-grid -f rhdg-chart/default-values.yaml
----


If you want to customize the server deployment (The `infinispan.yaml` file), you will need to provide server configuration in YAML format. You can use the following examples:

* `rhdg-chart/default-values.yaml`: Example provided in the https://github.com/infinispan/infinispan-helm-charts/blob/0.2.0/values.yaml[Helm Charts GitHub repository].
* `extras/config/server-example-infinispan.yaml`: Example provided as a testing file for the server in the https://github.com/infinispan/infinispan/blob/13.0.2.Final/server/runtime/src/test/resources/configuration/ServerConfigurationParserTest.yaml[Infinispan GitHub repository]. 
* `extras/config/default-operator-infinispan.yaml`: Example obtained from the RHDG image for Infinispan 13.0.2.Final.
* https://infinispan.org/docs/infinispan-operator/main/operator.html#infinispan-configuration_configuring-clusters[Upstream documentation] with examples.







=== Option 2: Customizing the official Helm Chart

To customize the Helm Chart, you will need to fork the official upstream chart adn modify the configuration needed. In my case, I have forked it https://github.com/alvarolop/infinispan-helm-charts[here], in order to change several aspects of the configuration.



1. Clone your own git repo in the parent folder:
[source, bash]
----
cd ..
git clone https://github.com/alvarolop/infinispan-helm-charts.git
cd infinispan-helm-charts
----
+
2. Create a new OCP project:
[source, bash]
----
oc new-project rhdg8-helm-customized --display-name="RHDG 8 - Helm Customized" --description="This namespace contains a deployment of RHDG using a customized Helm Chart"
----
+
3. In order to deploy this unpackaged version of the Helm Chart, you just have to use Helm to render the OCP objects using the default values file and apply the result in your OCP cluster:
[source, bash]
----
helm template --validate --set deploy.nameOverride="infinispan" . | oc apply -f -
----

Alternatively, you can use the `values.yaml` files defined in this repository: 
[source, bash]
----
helm template --validate --set deploy.nameOverride="infinispan" -f ../rhdg8-server/rhdg-chart/default-values.yaml . | oc apply -f -
----

[NOTE]
====
In the previous commands, you need the following parameters:
* `--validate`: By default, `helm template` does not validate your manifests against the Kubernetes cluster you are currently pointing at. You need to force it. (`Helm install` does validate by default, that is why this param is only necessary in this section).
* `--set deploy.nameOverride="infinispan"`: By default, the packaged Helm Chart uses the name of the package `infinispan`. As this is not the packaged version, the name defaults to RELEASE-NAME which is not a lowercase RFC 1123 subdomain. 
====



For more information, check the following links:

* https://infinispan.org/docs/helm-chart/main/helm-chart.html[Upstream documentation].
* https://github.com/infinispan/infinispan-helm-charts[Upstream Helm Chart source code].
* https://github.com/openshift-helm-charts/charts/tree/main/charts/redhat/redhat/datagrid[Packaged chart Downstream].
* https://github.com/openshift-helm-charts/charts/tree/main/charts/community/infinispan/infinispan[Pachaged chart Upstream].












== Monitoring RHDG with Grafana

A typical OpenShift monitoring stack includes Prometheus for monitoring both systems and services, and Grafana for analyzing and visualizing metrics.

Administrators are often looking to write custom queries and create custom dashboards in Grafana. However, Grafana instances provided with the monitoring stack (and its dashboards) are read-only.  To solve this problem, we can use the community-powered Grafana operator provided by OperatorHub.

To deploy the community-powered Grafana operator on OCP 4.9 just follow these steps:

=== Deploy the Grafana operator
[source, bash]
----
oc process -f grafana/grafana-01-operator.yaml | oc apply -f -
----

=== Create a Grafana instance
Now, we will create a Grafana instance using the operator:
[source, bash]
----
oc process -f grafana/grafana-02-instance.yaml | oc apply -f -
----

=== Create a Grafana data source
Now, we will create a Grafana data source:
[source, bash]
----
PROJECT=grafana

oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount -n ${PROJECT}
BEARER_TOKEN=$(oc serviceaccounts get-token grafana-serviceaccount -n ${PROJECT})
oc process -f grafana/grafana-03-datasource.yaml -p BEARER_TOKEN=${BEARER_TOKEN} | oc apply -f -
----

=== Create a Grafana dashboard
Now, we will create a Grafana dashboard:
[source, bash]
----
DASHBOARD_NAME="grafana-dashboard-rhdg8"
# Create a configMap containing the Dashboard
oc create configmap $DASHBOARD_NAME --from-file=dashboard=grafana/$DASHBOARD_NAME.json -n $PROJECT
# Create a Dashboard object that automatically updates Grafana
oc process -f grafana/grafana-04-dashboard.yaml -p DASHBOARD_NAME=$DASHBOARD_NAME | oc apply -f -
----

NOTE: https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/dashboards[Here] you can find information on other ways of creating dashboards.


=== Access Grafana as admin

After accessing Grafana using the OCP SSO, you may log in as `admin`. Retrieve the credentials from the secret using the following commands:
[source, bash]
----
oc get secret grafana-admin-credentials -n $PROJECT -o jsonpath='{.data.GF_SECURITY_ADMIN_USER}' | base64 --decode
oc get secret grafana-admin-credentials -n $PROJECT -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' | base64 --decode
----



For more information, access the Grafana https://grafana.com/docs/grafana/latest/[main documentation] or the Grafana https://github.com/integr8ly/grafana-operator/blob/v3.6.0/README.md[operator documentation].




== Alerting with PrometheusRules


[source, bash]
----
oc process -f rhdg-operator/rhdg-05-alerting-rules.yaml | oc apply -f -
----




:sectnums!:


== Annex: Configure the scope of your operator

An Operator group, defined by the OperatorGroup resource, provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators.

If you want to modify the default behavior of the template provided in this repository, modify lines 26 to 33 of this link:rhdg/rhdg-01-operator.yaml[template].

1) *AllNamespaces*: The Operator can be a member of an Operator group that selects all namespaces (target namespace set is the empty string ""). This configuration allows us to create DG clusters in every namespace of the cluster:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec: {}
----

2) *MultiNamespace*: The Operator can be a member of an Operator group that selects more than one namespace. Choose this option if you want to have several operators that manage RHDG clusters. For example, if you want to have a different operator per Business Unit managing several Openshift projects:
[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE-1}
      - ${CLUSTER_NAMESPACE-2}
----

3) *SingleNamespace*: The Operator can be a member of an Operator group that selects one namespace. This is useful if we want every application (Each OCP namespace) to be able to configure and deploy its own DG clusters:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE}
----

For more information, check the Openshift https://docs.openshift.com/container-platform/4.9/operators/understanding/olm/olm-understanding-operatorgroups.html#olm-operatorgroups-membership_olm-understanding-operatorgroups[documentation] about Operator Groups and the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/guide/fd77665b-d6df-4e25-a9cd-45fbed6dd6c1[documentation] to install DG on Openshift.



== Annex: Stern - Tail logs from multiple pods

In some situations, you will need to monitor logs from several pods of the same application and maybe you want to check to which pod the request arrived. https://github.com/wercker/stern[Stern] allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.

First, you will need to install it on your machine. After that, log in to your cluster and monitoring the previous deployment is as simple as executing the following command:

[source, bash]
----
stern --namespace=$CLUSTER_NAMESPACE -l clusterName=$CLUSTER_NAME
----

The previous command will show all the logs from all the pods from a namespace that contains a given label. There are many filters and configuration options. Check the https://github.com/wercker/stern#cli-flags[documentation] for a full list of them










== Annex: Advanced stats, and reporting for RHDG


=== Retrieve Queries stats

Since Infinispan 12.0, Data Grid https://infinispan.org/docs/stable/titles/query/query.html#getting-query-statistics_query-monitoring-tuning[includes metrics] specifically related to Queries on the server side. Retrieve them using the following script:

[source, bash]
----
CACHE_NAME="operator-cache-01"
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Get stats"
  oc exec $pod -- bash -c 'curl $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN/rest/v2/caches/$CACHE_NAME/search/stats' | jq
done
----


=== Retrieve server reports from Openshift

Since Infinispan 12.0, Data Grid includes an option to https://github.com/infinispan/infinispan/blob/13.0.0.Final/server/runtime/src/main/server/bin/report.sh[download a server report] from each pod. Retrieve it using the following script:

[source, bash]
----
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Generate report"
  oc exec $pod -- bash -c 'echo "server report" | ./bin/cli.sh -c $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN -f -'
  echo "$pod: Download report"
  oc exec $pod -- bash -c 'files=( *tar.gz* ); cat "${files[0]}"' > $(date +"%Y-%m-%d-%H-%M")-$pod-report.tar.gz
  echo "$pod: Remove report"
  oc exec rhdg-0 -- bash -c 'rm -rf *tar.gz*'
done
----



== Annex: Convert cache configurations

In Data Grid 7, caches were defined in XML format. Since RHDG 8, it is possible to use XML, JSON or YAML. The server includes some tools to automatically convert from one to the other.


=== Option 1: Cache already exists in the cluster

[source, bash]
----
CACHE_NAME="___protobuf_metadata"
# Get in XML
curl --digest -u developer:$DEV_PASS -H "Accept: application/xml" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
# Get in JSON
curl --digest -u developer:$DEV_PASS -H "Accept: application/json" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
# Get in YAML
curl --digest -u developer:$DEV_PASS -H "Accept: application/yaml" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
----


=== Option 2: The cache is not in the cluster

The following example converts an XML definition to YAML:
[source, bash]
----
curl localhost:11222/rest/v2/caches?action=convert\
  --digest -u developer:developer \
  -X POST \
  -H "Accept: application/yaml" \
  -H "Content-Type: application/xml" \
  -d '<?xml version="1.0" encoding="UTF-8"?><replicated-cache mode="SYNC" statistics="false"><encoding media-type="application/x-protostream"/><expiration lifespan="300000" /><memory max-size="400MB" when-full="REMOVE"/><state-transfer enabled="true" await-initial-transfer="false"/></replicated-cache>'
----

The result is the following YAML:

[source, yaml]
----
replicatedCache:
  mode: "SYNC"
  statistics: "false"
  encoding:
    key:
      mediaType: "application/x-protostream"
    value:
      mediaType: "application/x-protostream"
  expiration:
    lifespan: "300000"
  memory:
    maxSize: "400MB"
    whenFull: "REMOVE"
  stateTransfer:
    enabled: "true"
    awaitInitialTransfer: "false"
----



== Annex: Getting full CR example

1. Download the Infinispan CRD:
+
[source, bash]
----
# Infinispan Operator 2.1.X
URL="https://raw.githubusercontent.com/infinispan/infinispan-operator/2.1.x/deploy/crds/infinispan.org_infinispans_crd.yaml"

# Infinispan Operator 2.2.X
URL="https://raw.githubusercontent.com/infinispan/infinispan-operator/2.2.x/config/crd/bases/infinispan.org_infinispans.yaml"

curl -o rhdg-crds/infinispan-2.2.x.yaml $URL
----
+
2. Edit the file in order to create a new CRD instead of modifying the previous one.
+
3. Create the object in the cluster:
+
[source, bash]
----
oc apply -f rhdg-crds/infinispan-2.2.x.yaml
----
+
4. Get the full list of options:
+
[source, bash]
----
oc explain custominfinispan --recursive
----






== Useful links

* https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/[RHDG 8.3 documentation].
* https://infinispan.org/documentation[Upstream documentation].
* https://access.redhat.com/articles/4933551[RHDG 8 Supported Configurations].
* https://access.redhat.com/articles/4933371[RHDG 8 Component Details].
* https://access.redhat.com/articles/4961121[RHDG 8 Maintenance Schedule].
* https://access.redhat.com/support/policy/updates/jboss_notes/#p_rhdg[RHDG Product Update and Support Policy].
* https://developers.redhat.com/blog/2020/10/15/securely-connect-quarkus-and-red-hat-data-grid-on-red-hat-openshift[Securely connect Quarkus and RHDG 8.1 on OCP].