= Red Hat Data Grid 8 server
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2020-11
// Metadata
:description: This document shows how to perform basic installations of Red Hat Data Grid customizing its configuration.
:keywords: infinispan, datagrid, openshift, red hat
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons


This repository demonstrates some of the basic features of Red Hat's latest Data Grid release and how to deploy a Data Grid cluster locally and on OCP. 

// Create the Table of contents here
toc::[]

== Introduction

Red Hat Data Grid is an in-memory, distributed, NoSQL datastore solution. Your applications can access, process, and analyze data at in-memory speed to deliver a superior user experience. 

Red Hat Data Grid provides value as a standard architectural component in application infrastructures for a variety of real-world scenarios and usecases:

* Data caching and transient data storage.
* Primary data store.
* Low latency compute grid.




== Deploying standalone

Currently, Data Grid is supported both using the binary on VMs and using the container images on Openshift. Due to the huge benefits of deploying our applications containerized in Openshift, we will use the same image to launch the Data Grid cluster locally using `podman-compose`:


.Launch the Data Grid cluster with only one replica
[source, bash]
----
podman-compose -f rhdg-local/docker-compose-1.yaml up
----

.Launch the Data Grid cluster with two replicas
[source, bash]
----
podman-compose -f rhdg-local/docker-compose-2.yaml up
----

If the cluster formation goes well, you will see that the JGroups Cluster View is updated to the two servers:

.Launch the server
[source, bash, subs="attributes"]
----
[eu-01] | 2023-06-27 07:48:06,213 INFO  (non-blocking-thread-infinispan-europe-01-p2-t1) [org.infinispan.LIFECYCLE] [Context=___hotRodTopologyCache_hotrod-default][Scope=infinispan-europe-01]ISPN100010: Finished rebalance with members [infinispan-europe-02, infinispan-europe-01], topology id 7
[eu-02] | 2023-06-27 07:48:06,224 INFO  (jgroups-5,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100009: Advancing to rebalance phase READ_ALL_WRITE_ALL, topology id 8
[eu-02] | 2023-06-27 07:48:06,227 INFO  (jgroups-6,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100009: Advancing to rebalance phase READ_NEW_WRITE_ALL, topology id 9
[eu-02] | 2023-06-27 07:48:06,230 INFO  (jgroups-5,infinispan-europe-02) [org.infinispan.CLUSTER] [Context=___hotRodTopologyCache_hotrod-default]ISPN100010: Finished rebalance with members [infinispan-europe-02, infinispan-europe-01], topology id 10
//... 
[eu-01] | 2023-06-27 07:48:06,401 INFO  (main) [org.infinispan.SERVER] ISPN080001: Infinispan Server 14.0.1.Final started in 2777ms
----

NOTE: For ways of deploying the server using the .zip, check the https://github.com/alvarolop/rhdg8-server/tree/rhdg84-rhel[following version of this repository].





== Deploying RHDG on OCP using the Operator

Install Data Grid Operator into an OpenShift namespace to create and manage Data Grid clusters.

=== Deploying the RHDG operator

To deploy the RHDG operator, you will need to create four objects: The namespace of the Operator, the namespace where the Data Grid cluster will live, the Subscription and the Operator Group.


IMPORTANT: Bear in mind that you will need `cluster-admin` permissions to deploy an operator, as it is necessary to create cluster-wide CRDs (Custom Resource Definitions).

Execute the following command to deploy the operator: 

[source, bash]
----
oc process -f rhdg-operator/rhdg-01-operator.yaml -p OPERATOR_NAMESPACE="rhdg8-operator" -p CLUSTER_NAMESPACE="rhdg8" | oc apply -f -
----

It is also possible to install the operator from the web console. For more information, please check the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_operator_guide/installation[documentation].


=== Deploying an RHDG cluster

The Data Grid operator defines the *Infinispan* CRD to simplify the process of creation, configuration, and management of DG clusters.


I have created several OCP templates to quickly deploy different set-ups of Data Grid:

[cols="4*",options="header",width=100%]
|===
|Template
|Authentication
|Authorization
|SSL Encryption

| link:rhdg-operator/rhdg-02-cluster-basic.yaml[Basic]
|{no}
|{no}
|{no}

| link:rhdg-operator/rhdg-02-cluster-auth.yaml[Auth]
|{yes}
|{yes}
|{no}

| link:rhdg-operator/rhdg-02-cluster-auth-ssl.yaml[SSL Encryption]
|{yes}
|{yes}
|{yes}

|===


Execute the following commands depending on the cluster that you want to deploy: 


[source, bash]
----
oc process -f rhdg-operator/rhdg-02-cluster-basic.yaml -p CLUSTER_NAME="cluster-a" | oc apply -f -

oc process -f rhdg-operator/rhdg-02-cluster-auth.yaml -p CLUSTER_NAME="cluster-b" | oc apply -f -

oc process -f rhdg-operator/rhdg-02-cluster-auth-ssl.yaml -p CLUSTER_NAME="cluster-c" | oc apply -f -
----



=== Creating RHDG caches using the Operator CRD

NOTE: In this section, we will explore how to create caches using the Operator. For other ways of creating caches, please check this other https://github.com/alvarolop/rhdg8-client#cache-configuration[Git repository] with information about the Data Grid client.

I have created an OCP template to quickly set up several caches on the RHDG cluster:

* `operator-cache-01`: Based on an XML configuration.
* `operator-cache-02`: Based on a YAML configuration.
* `operator-cache-03`: Based on an already defined templated.


In order to apply this template, just execute the following command:

[source, bash]
----
oc process -f rhdg-operator/rhdg-03-caches.yaml -p CLUSTER_NAMESPACE="rhdg8" -p CLUSTER_NAME="rhdg" | oc apply -f -
----


For more information about how to create caches using the CRD, please check the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_operator_guide/creating-caches[official documentation].





=== Monitoring RHDG with Prometheus

Data Grid exposes a metrics endpoint that provides statistics and events in Prometheus format.


.Enabling monitoring for user-defined projects
[WARNING]
==== 
Do not execute it before checking if this was done before, you can override work from your colleagues:

[source, bash]
----
oc apply -f ocp/ocp-01-user-workload-monitoring.yaml
----

After executing the command above, you will see several pods in the following namespace:
[source, bash]
----
oc get pods -n openshift-user-workload-monitoring
----
====

// In order to access the Prometheus that will contain the metrics of DG, expose its service:
// [source, bash]
// ----
// oc expose svc/prometheus-user-workload -n openshift-user-workload-monitoring
// ----


I have created an OCP template to quickly configure metrics monitoring of an RHDG cluster. Execute the following command:

[source, bash]
----
oc process -f rhdg-operator/rhdg-04-monitoring.yaml -p CLUSTER_NAMESPACE="rhdg8" -p CLUSTER_NAME="rhdg" | oc apply -f -
----


For more information, access the Openshift https://docs.openshift.com/container-platform/4.13/monitoring/enabling-monitoring-for-user-defined-projects.html[documentation] for the monitoring stack and the RHDG documentation to https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_operator_guide/monitoring-services[configure monitoring] for RHDG 8 on OCP.



=== Alerting with PrometheusRules


[source, bash]
----
oc process -f rhdg-operator/rhdg-05-alerting-rules.yaml -p CLUSTER_NAMESPACE="rhdg8" | oc apply -f -
----






== Deploying RHDG on OCP using Helm Charts


=== Option 1: Using an official Infinispan Helm Chart release

In order to create your first deployment easily, first https://github.com/openshift-helm-charts/charts/blob/main/README.md[add the OpenShift Helm Charts repository]:


[source, bash]
----
helm repo add openshift-helm-charts https://charts.openshift.io/
----

Create a new OCP project:
[source, bash]
----
oc new-project rhdg8-helm --display-name="RHDG 8 - Helm" --description="This namespace contains a deployment of RHDG using the official Helm Chart"
----

Then, modify the `rhdg-chart/default-values.yaml` to configure your deployment:
[source, bash]
----
helm install rhdg openshift-helm-charts/redhat-data-grid -f rhdg-chart/minimal-values.yaml
----

You will be able to authenticate to the cluster using the credentials obtained from the following command:
[source, bash]
----
oc get secret rhdg-generated-secret \
-o jsonpath="{.data.identities-batch}" | base64 --decode
----

If you want to make changes, you need to update the values file and use the `helm upgrade` command:
[source, bash]
----
helm upgrade rhdg openshift-helm-charts/redhat-data-grid -f rhdg-chart/default-values.yaml
----


If you want to customize the server deployment - the `infinispan.yaml` file -, you will need to provide server configuration in YAML format. You can use the following examples:

* `rhdg-chart/default-values.yaml`: Example provided in the https://github.com/infinispan/infinispan-helm-charts/blob/0.3.1/values.yaml[Helm Charts GitHub repository].
* `extras/config/server-example-infinispan.yaml`: Example provided as a testing file for the server in the https://github.com/infinispan/infinispan/blob/13.0.2.Final/server/runtime/src/test/resources/configuration/ServerConfigurationParserTest.yaml[Infinispan GitHub repository]. 
* `extras/config/default-operator-infinispan.yaml`: Example obtained from the RHDG image for Infinispan 13.0.2.Final.
* https://infinispan.org/docs/infinispan-operator/main/operator.html#infinispan-configuration_configuring-clusters[Upstream documentation] with examples.







=== Option 2: Customizing the official Helm Chart

To customize the Helm Chart, you will need to fork the official upstream chart and modify the configuration needed:


1. Clone your git repo in the parent folder:
+
[source, bash]
----
git clone https://github.com/alvarolop/infinispan-helm-charts.git
cd infinispan-helm-charts
----
+
2. Create a new OCP project:
+
[source, bash]
----
oc new-project rhdg8-helm-customized --display-name="RHDG 8 - Helm Customized" --description="This namespace contains a deployment of RHDG using a customized Helm Chart"
----
+
3. In order to deploy this unpackaged version of the Helm Chart, you just have to use Helm to render the OCP objects using the default values file and apply the result in your OCP cluster:
+
[source, bash]
----
helm template --validate --set deploy.nameOverride="infinispan" . | oc apply -f -
----

Alternatively, you can use the `values.yaml` files defined in this repository: 

[source, bash]
----
helm template --validate --set deploy.nameOverride="infinispan" -f ../rhdg8-server/rhdg-chart/default-values.yaml . | oc apply -f -
----

[NOTE]
====
In the previous commands, you need the following parameters:
* `--validate`: By default, `helm template` does not validate your manifests against the Kubernetes cluster you are currently pointing at. You need to force it. (`Helm install` does validate by default, that is why this param is only necessary for this section).
* `--set deploy.nameOverride="infinispan"`: By default, the packaged Helm Chart uses the name of the package `infinispan`. As this is not the packaged version, the name defaults to RELEASE-NAME which is not a lowercase RFC 1123 subdomain. 
====



For more information, check the following links:

* https://infinispan.org/docs/helm-chart/main/helm-chart.html[Upstream documentation].
* https://github.com/infinispan/infinispan-helm-charts[Upstream Helm Chart source code].
* https://github.com/openshift-helm-charts/charts/tree/main/charts/redhat/redhat/datagrid[Packaged chart Downstream].
* https://github.com/openshift-helm-charts/charts/tree/main/charts/community/infinispan/infinispan[Pachaged chart Upstream].












== Monitoring RHDG with Grafana

A typical OpenShift monitoring stack includes Prometheus - for monitoring both systems and services-, and Grafana - for analyzing and visualizing metrics-.

To deploy the community-powered Grafana operator on OCP 4.13 just follow these steps:

=== Deploy the Grafana operator
[source, bash]
----
oc process -f grafana/grafana-01-operator.yaml | oc apply -f -
----

=== Create a Grafana instance
Now, we will create a Grafana instance using the operator:
[source, bash]
----
oc process -f grafana/grafana-02-instance.yaml | oc apply -f -
----

=== Create a Grafana data source
Now, we will create a Grafana data source:
[source, bash]
----
GRAFANA_NAMESPACE=grafana

oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount -n ${GRAFANA_NAMESPACE}
BEARER_TOKEN=$(oc get secret $(oc describe sa grafana-serviceaccount -n $GRAFANA_NAMESPACE | awk '/Tokens/{ print $2 }') -n $GRAFANA_NAMESPACE --template='{{ .data.token | base64decode }}')
oc process -f grafana/grafana-03-datasource.yaml -p BEARER_TOKEN=${BEARER_TOKEN} | oc apply -f -
----

=== Create a Grafana dashboard
Now, we will create a Grafana dashboard:
[source, bash]
----
DASHBOARD_NAME="grafana-dashboard-rhdg8"
# Create a configMap containing the Dashboard
oc create configmap $DASHBOARD_NAME --from-file=dashboard=grafana/$DASHBOARD_NAME.json -n $GRAFANA_NAMESPACE
# Create a Dashboard object that automatically updates Grafana
oc process -f grafana/grafana-04-dashboard.yaml -p DASHBOARD_NAME=$DASHBOARD_NAME | oc apply -f -
----

NOTE: https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/dashboards[Here] you can find information on other ways of creating dashboards.


=== Access Grafana as admin

After accessing Grafana using the OCP SSO, you may log in as `admin`. Retrieve the credentials from the secret using the following commands:
[source, bash]
----
oc get secret grafana-admin-credentials -n $GRAFANA_NAMESPACE -o jsonpath='{.data.GF_SECURITY_ADMIN_USER}' | base64 --decode
oc get secret grafana-admin-credentials -n $GRAFANA_NAMESPACE -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' | base64 --decode
----



For more information, access the Grafana https://grafana.com/docs/grafana/latest/[main documentation] or the Grafana https://github.com/grafana-operator/grafana-operator/blob/v4.10.1/README.md[operator documentation].









:sectnums!:




== Annex: Interact with caches using REST


Interact with the newly created caches with the following commands:
[source, bash]
----
# Set your variables. These are default:
CLUSTER_NAMESPACE="rhdg8"
CLUSTER_NAME="rhdg"
RHDG_URL=$(oc get route ${CLUSTER_NAME}-external -n ${CLUSTER_NAMESPACE} -o template='https://{{.spec.host}}')

# Check all the caches on your cluster
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches | jq

# Check information about an specific cache
curl -X GET -k -u developer:developer -H "Content-Type: application/json" ${RHDG_URL}/rest/v2/caches/${CACHE_NAME} | jq

# Delete a cache
curl -X DELETE -k -u developer:developer ${RHDG_URL}/rest/v2/caches/${CACHE_NAME}
----


== Annex: Configure the scope of your operator

An Operator group, defined by the OperatorGroup resource, provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators.

If you want to modify the default behavior of the template provided in this repository, modify lines 26 to 33 of this link:rhdg/rhdg-01-operator.yaml[template].

1) *AllNamespaces*: The Operator can be a member of an Operator group that selects all namespaces (target namespace set is the empty string ""). This configuration allows us to create DG clusters in every namespace of the cluster:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec: {}
----

2) *MultiNamespace*: The Operator can be a member of an Operator group that selects more than one namespace. Choose this option if you want to have several operators that manage RHDG clusters. For example, if you want to have a different operator per Business Unit managing several Openshift projects:
[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE-1}
      - ${CLUSTER_NAMESPACE-2}
----

3) *SingleNamespace*: The Operator can be a member of an Operator group that selects one namespace. This is useful if we want every application (Each OCP namespace) to be able to configure and deploy its own DG clusters:

[source, yaml]
----
- apiVersion: operators.coreos.com/v1
  kind: OperatorGroup
  metadata:
    name: datagrid
    namespace: ${OPERATOR_NAMESPACE}
  spec:
    targetNamespaces:
      - ${CLUSTER_NAMESPACE}
----

For more information, check the Openshift https://docs.openshift.com/container-platform/4.9/operators/understanding/olm/olm-understanding-operatorgroups.html#olm-operatorgroups-membership_olm-understanding-operatorgroups[documentation] about Operator Groups and the official https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/guide/fd77665b-d6df-4e25-a9cd-45fbed6dd6c1[documentation] to install DG on Openshift.



== Annex: Stern - Tail logs from multiple pods

In some situations, you will need to monitor logs from several pods of the same application and maybe you want to check to which pod the request arrived. https://github.com/wercker/stern[Stern] allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.

First, you will need to install it on your machine. After that, log in to your cluster and monitoring the previous deployment is as simple as executing the following command:

[source, bash]
----
stern --namespace=$CLUSTER_NAMESPACE -l clusterName=$CLUSTER_NAME
----

The previous command will show all the logs from all the pods from a namespace that contains a given label. There are many filters and configuration options. Check the https://github.com/wercker/stern#cli-flags[documentation] for a full list of them










== Annex: Advanced stats, and reporting for RHDG


=== Retrieve Queries stats

Since Infinispan 12.0, Data Grid https://infinispan.org/docs/stable/titles/query/query.html#getting-query-statistics_query-monitoring-tuning[includes metrics] specifically related to Queries on the server side. Retrieve them using the following script:

[source, bash]
----
CACHE_NAME="operator-cache-01"
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Get stats"
  oc exec $pod -- bash -c 'curl $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN/rest/v2/caches/$CACHE_NAME/search/stats' | jq
done
----


=== Retrieve server reports from Openshift

Since Infinispan 12.0, Data Grid includes an option to https://github.com/infinispan/infinispan/blob/13.0.0.Final/server/runtime/src/main/server/bin/report.sh[download a server report] from each pod. Retrieve it using the following script:

[source, bash]
----
oc project $RHDG_NAMESPACE
for pod in $(oc get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
  echo "$pod: Generate report"
  oc exec $pod -- bash -c 'echo "server report" | ./bin/cli.sh -c $HOSTNAME:$RHDG_SERVICE_PORT_INFINISPAN -f -'
  echo "$pod: Download report"
  oc exec $pod -- bash -c 'files=( *tar.gz* ); cat "${files[0]}"' > $(date +"%Y-%m-%d-%H-%M")-$pod-report.tar.gz
  echo "$pod: Remove report"
  oc exec rhdg-0 -- bash -c 'rm -rf *tar.gz*'
done
----



== Annex: Convert cache configurations

In Data Grid 7, caches were defined in XML format. Since RHDG 8, it is possible to use XML, JSON or YAML. The server includes some tools to automatically convert from one to the other.


=== Option 1: Cache already exists in the cluster

[source, bash]
----
CACHE_NAME="___protobuf_metadata"
# Get in XML
curl --digest -u developer:$DEV_PASS -H "Accept: application/xml" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
# Get in JSON
curl --digest -u developer:$DEV_PASS -H "Accept: application/json" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
# Get in YAML
curl --digest -u developer:$DEV_PASS -H "Accept: application/yaml" $INFINISPAN_SERVICE_HOST:11222/rest/v2/caches/$CACHE_NAME?action=config
----


=== Option 2: The cache is not in the cluster

The following example converts an XML definition to YAML:
[source, bash]
----
curl localhost:11222/rest/v2/caches?action=convert\
  --digest -u developer:developer \
  -X POST \
  -H "Accept: application/yaml" \
  -H "Content-Type: application/xml" \
  -d '<?xml version="1.0" encoding="UTF-8"?><replicated-cache mode="SYNC" statistics="false"><encoding media-type="application/x-protostream"/><expiration lifespan="300000" /><memory max-size="400MB" when-full="REMOVE"/><state-transfer enabled="true" await-initial-transfer="false"/></replicated-cache>'
----

The result is the following YAML:

[source, yaml]
----
replicatedCache:
  mode: "SYNC"
  statistics: "false"
  encoding:
    key:
      mediaType: "application/x-protostream"
    value:
      mediaType: "application/x-protostream"
  expiration:
    lifespan: "300000"
  memory:
    maxSize: "400MB"
    whenFull: "REMOVE"
  stateTransfer:
    enabled: "true"
    awaitInitialTransfer: "false"
----



== Annex: Getting full CR example

1. Download the Infinispan CRD:
+
[source, bash]
----
# Infinispan Operator 2.1.X
URL="https://raw.githubusercontent.com/infinispan/infinispan-operator/2.1.x/deploy/crds/infinispan.org_infinispans_crd.yaml"

# Infinispan Operator 2.2.X
URL="https://raw.githubusercontent.com/infinispan/infinispan-operator/2.2.x/config/crd/bases/infinispan.org_infinispans.yaml"

curl -o rhdg-crds/infinispan-2.2.x.yaml $URL
----
+
2. Edit the file in order to create a new CRD instead of modifying the previous one.
+
3. Create the object in the cluster:
+
[source, bash]
----
oc apply -f rhdg-crds/infinispan-2.2.x.yaml
----
+
4. Get the full list of options:
+
[source, bash]
----
oc explain custominfinispan --recursive
----






== Useful links

* https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/[RHDG 8.3 documentation].
* https://infinispan.org/documentation[Upstream documentation].
* https://access.redhat.com/articles/4933551[RHDG 8 Supported Configurations].
* https://access.redhat.com/articles/4933371[RHDG 8 Component Details].
* https://access.redhat.com/articles/4961121[RHDG 8 Maintenance Schedule].
* https://access.redhat.com/support/policy/updates/jboss_notes/#p_rhdg[RHDG Product Update and Support Policy].
* https://developers.redhat.com/blog/2020/10/15/securely-connect-quarkus-and-red-hat-data-grid-on-red-hat-openshift[Securely connect Quarkus and RHDG 8.1 on OCP].